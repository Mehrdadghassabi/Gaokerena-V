{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C9pvAISwrg6",
        "outputId": "e3f7ec6b-98f2-40d2-ab78-dd9b83a9a9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp\n",
            "To: /content/MedNLI_dataset.zip\n",
            "100% 681k/681k [00:00<00:00, 86.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yKyuDtXy1a2NJ-I7H4LpR-LpWSdw09P0\n",
            "To: /content/MedNLI_fa_dataset.zip\n",
            "100% 1.88M/1.88M [00:00<00:00, 188MB/s]\n",
            "Archive:  MedNLI_dataset.zip\n",
            "replace MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet  \n",
            "replace MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet  \n",
            "replace MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet  \n",
            "Archive:  MedNLI_fa_dataset.zip\n",
            "   creating: MedNLI_fa_dataset/\n",
            "  inflating: MedNLI_fa_dataset/mednlidev_en.xlsx  \n",
            "  inflating: MedNLI_fa_dataset/mednlidev_fa.xlsx  \n",
            "  inflating: MedNLI_fa_dataset/mednlitest_en.xlsx  \n",
            "  inflating: MedNLI_fa_dataset/mednlitest_fa.xlsx  \n",
            "  inflating: MedNLI_fa_dataset/mednlitrain_en.xlsx  \n",
            "  inflating: MedNLI_fa_dataset/mednlitrain_fa.xlsx  \n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.10/dist-packages (2024.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.26.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2024.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp'\n",
        "!gdown --id '1yKyuDtXy1a2NJ-I7H4LpR-LpWSdw09P0'\n",
        "!unzip MedNLI_dataset.zip\n",
        "!unzip MedNLI_fa_dataset.zip\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "CHyIKncLxwF7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_test_data = pd.read_parquet('MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet', engine='fastparquet')\n",
        "en_train_data = pd.read_parquet('MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet', engine='fastparquet')\n",
        "en_valid_data = pd.read_parquet('MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet', engine='fastparquet')\n",
        "\n",
        "fa_test_data = pd.read_excel('MedNLI_fa_dataset/mednlitest_fa.xlsx')\n",
        "fa_train_data = pd.read_excel('MedNLI_fa_dataset/mednlitrain_fa.xlsx')\n",
        "fa_valid_data = pd.read_excel('MedNLI_fa_dataset/mednlidev_fa.xlsx')"
      ],
      "metadata": {
        "id": "bUm7p8mhxuUa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pre_and_hyp(query):\n",
        "    start_pre = query.find(\"[PRE]\") + len(\"[PRE]\")\n",
        "    end_pre = query.find(\"[HYP]\")\n",
        "    start_hyp = query.find(\"[HYP]\") + len(\"[HYP]\")\n",
        "    end_hyp = query.find(\"OUTPUT:\")\n",
        "    premise = query[start_pre:end_pre].strip()\n",
        "    hypothesis = query[start_hyp:end_hyp].strip()\n",
        "\n",
        "    return premise,hypothesis"
      ],
      "metadata": {
        "id": "uMni08wKx0md"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpFHMQmzzPAK",
        "outputId": "35873a97-711f-47c7-fe62-ff0a03c7cb4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_indx(dictionary):\n",
        "    if not dictionary:\n",
        "        return None\n",
        "    max_key = max(dictionary, key=dictionary.get)\n",
        "    return max_key"
      ],
      "metadata": {
        "id": "8Aejic1G0AZl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(premise,hypothesis):\n",
        "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "         output = model(input[\"input_ids\"].to(device))\n",
        "         prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "         label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "         prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
        "    return get_max_indx(prediction)"
      ],
      "metadata": {
        "id": "pOY6IJcRzlkW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_acc(model,datas):\n",
        "    sum = 0\n",
        "    count = 0\n",
        "    for index,row in datas.iterrows():\n",
        "        pre,hyp = find_pre_and_hyp(row['query'])\n",
        "        ground_truth = row['answer']\n",
        "        model_answer = inference(pre,hyp)\n",
        "\n",
        "        count += 1\n",
        "        if model_answer ==  ground_truth:\n",
        "           sum += 1\n",
        "    return (sum/count)"
      ],
      "metadata": {
        "id": "vhPrX8gP0USS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print('Accuracy on english is: ' + str(get_model_acc(model,en_test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNxb6lwa0jQK",
        "outputId": "f3c09027-e7e8-4eef-85fe-6be491760761"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on english is: 0.670182841068917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print('Accuracy on persian is: ' + str(get_model_acc(model,fa_test_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxUpBllJATqI",
        "outputId": "17aa73f4-b2bc-4551-a024-acf0732306a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on persian is: 0.6385372714486639\n"
          ]
        }
      ]
    }
  ]
}