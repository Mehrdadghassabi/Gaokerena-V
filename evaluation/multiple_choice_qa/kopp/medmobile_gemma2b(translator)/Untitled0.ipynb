{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B02h0Cc_sXk7",
        "outputId": "8296d3a4-a0f7-41e2-a60a-bc406b2e680b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiIi4MeTeDmj"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, MT5ForConditionalGeneration, MT5Tokenizer\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLplsAShucvo"
      },
      "outputs": [],
      "source": [
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gWK6-XCqrmj6"
      },
      "outputs": [],
      "source": [
        "# @title prompts\n",
        "prompt_get_part = f'''\n",
        "\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "Here is a medical question. Select one of the following topics that this question would best be classified as:\n",
        "\n",
        "Topic List = [\n",
        "    \"Part 1: The Profession of Medicine\",\n",
        "    \"Part 2: Cardinal Manifestations and Presentation of Diseases\",\n",
        "    \"Part 3: Pharmacology\",\n",
        "    \"Part 4: Oncology and Hematology\",\n",
        "    \"Part 5: Infectious Diseases\",\n",
        "    \"Part 6: Disorders of the Cardiovascular System\",\n",
        "    \"Part 7: Disorders of the Respiratory System\",\n",
        "    \"Part 8: Critical Care Medicine\",\n",
        "    \"Part 9: Disorders of the Kidney and Urinary Tract\",\n",
        "    \"Part 10: Disorders of the Gastrointestinal System\",\n",
        "    \"Part 11: Immune-Mediated, Inflammatory, and Rheumatologic Disorders\",\n",
        "    \"Part 12: Endocrinology and Metabolism\",\n",
        "    \"Part 13: Neurologic Disorders\",\n",
        "    \"Part 14: Poisoning, Drug Overdose, and Envenomation\",\n",
        "    \"Part 15: Disorders Associated with Environmental Exposures\",\n",
        "    \"Part 16: Genes, the Environment, and Disease\",\n",
        "    \"Part 17: Global Medicine\",\n",
        "    \"Part 18: Aging\",\n",
        "    \"Part 19: Consultative Medicine\",\n",
        "    \"Part 20: Frontiers\"\n",
        "]\n",
        "\n",
        "Respond with a number, 1-20 representing the part that the question most likely belongs in. Only respond with this number.\n",
        "'''\n",
        "\n",
        "prompt_eval = f'''\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "Here is a medical question, some answer choices. Think through the problem step by step and provide your selection like this:\n",
        "model generated chain of thought explanation. Therefore, the answer is [final model answer (e.g. A,B,C,D)] for instance [A].\n",
        "\\n\n",
        "'''\n",
        "\n",
        "prompt_eval_bare = f'''\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "\\n\n",
        "'''\n",
        "\n",
        "prompt_eval_bare_fully = f'''\n",
        "{{question}} \\n\n",
        "{{choices}}\n",
        "'''\n",
        "\n",
        "prompt_eval_bare_fully_with_examples = f'''\n",
        "{{examples}} \\n\n",
        "{{question}} \\n\n",
        "{{choices}}\n",
        "'''\n",
        "\n",
        "prompt_eval_with_examples = f'''\n",
        "## Examples\n",
        "{{examples}}\n",
        "\n",
        "Above are examples for medical Q&A.\n",
        "\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "Here is a medical question, some answer choices. Think through the problem step by step and provide your selection like this:\n",
        "model generated chain of thought explanation. Therefore, the answer is [final model answer (e.g. A,B,C,D)] for instance [A].\n",
        "\\n\n",
        "'''\n",
        "\n",
        "prompt_eval_context_bare = f'''\n",
        "{{context}} \\n\n",
        "{{question}} \\n\n",
        "{{choices}}\n",
        "'''\n",
        "prompt_eval_with_context = f'''\n",
        "## Context\n",
        "{{context}} \\n\n",
        "\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "Here is some context from a textbook, a medical question, some answer choices. Think through the problem step by step and provide your selection like this:\n",
        "model generated chain of thought explanation. Therefore, the answer is [final model answer (e.g. A,B,C,D)] for instance [A].\n",
        "\\n '''\n",
        "\n",
        "prompt_eval_with_context_and_examples = f'''\n",
        "## Examples\n",
        "{{examples}}\n",
        "\n",
        "Above are examples of medical Q&A.\n",
        "\n",
        "## Context\n",
        "{{context}} \\n\n",
        "\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "Here is some context from a textbook, a medical question, some answer choices. Think through the problem step by step and provide your selection like this:\n",
        "model generated chain of thought explanation. Therefore, the answer is [final model answer (e.g. A,B,C,D)] for instance [A].\n",
        "\\n '''\n",
        "\n",
        "prompt_example = f'''\n",
        "## Question\n",
        "{{question}} \\n\n",
        "\n",
        "## Choices\n",
        "{{choices}} \\n\n",
        "\n",
        "## Answer\n",
        "{{answer}} \\n\n",
        "'''\n",
        "\n",
        "gpt_chain_of_thoughts_MEDPROMPT = {\n",
        "    \"prompt_name\": \"gpt_chain_of_thoughts\",\n",
        "    \"response_type\": \"MC\",\n",
        "    \"examples\": [\n",
        "        {\n",
        "            \"question\": \"\"\"A 6-year-old girl is brought to the emergency department because of right elbow swelling and pain 30 minutes after falling onto her outstretched right arm. She has been unable to move her right elbow since the fall. Physical examination shows bruising, swelling, and tenderness of the right elbow; range of motion is limited by pain. An x-ray of the right arm shows a supracondylar fracture of the humerus with anterior displacement of the proximal fragment. Further evaluation is most likely to show which of the following findings?\n",
        "\n",
        "A. Absent distal radial pulse\n",
        "B. Radial deviation of the wrist\n",
        "C. Inability to abduct shoulder\n",
        "D. Inability to flex the elbow\n",
        "\"\"\",\n",
        "            \"answer\": \"\"\"A supracondylar fracture of the humerus, especially with anterior displacement of the proximal fragment, is a common and serious injury in children. It can cause injury to the brachial artery, leading to compromised blood flow to the forearm and hand. Therefore, the most likely additional finding in this case would be an absent distal radial pulse due to this potential vascular compromise. This is a serious complication and requires immediate orthopedic consultation and likely surgical intervention to prevent long-term damage.\n",
        "So, the answer is [A].\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"\"\"Two weeks after undergoing low anterior resection for rectal cancer, a 52-year-old man comes to the physician because of swelling in both feet. He has not had any fever, chills, or shortness of breath. His temperature is 36°C (96.8°F) and pulse is 88/min. Physical examination shows a normal thyroid and no jugular venous distention. Examination of the lower extremities shows bilateral non-pitting edema that extends from the feet to the lower thigh, with deep flexion creases. His skin is warm and dry, and there is no erythema or rash. Microscopic examination of the interstitial space in this patient's lower extremities would be most likely to show the presence of which of the following?\n",
        "\n",
        "A. Acellular, protein-poor fluid\n",
        "B. Lymphocytic, hemosiderin-rich fluid\n",
        "C. Lipid-rich, protein-rich fluid\n",
        "D. Protein-rich, glycosaminoglycan-rich fluid\n",
        "\"\"\",\n",
        "            \"answer\": \"\"\"This patient's bilateral, symmetric lower extremity swelling extending to the thighs, nonpitting quality, and absence of systemic symptoms (eg, shortness of breath, orthopnea) are suggestive of lymphedema. Lymphedema develops due to a blockage in the lymphatic system that prevents lymph fluid from draining adequately. In this case, the patient's recent surgery (low anterior resection for rectal cancer) probably involved extensive lymph node dissection, leading to secondary lymphedema.\n",
        "\n",
        "Microscopically, lymph is a protein-rich fluid that also contains lipids, lymphocytes, and other substances. It does not typically contain significant amounts of glycosaminoglycans or hemosiderin.\n",
        "\n",
        "Therefore, the answer is [C].\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"\"\"A 34-year-old gravida 2, para 1 woman at 37+6 weeks of gestation presents for elective cesarean delivery. She says she has been having increased fatigue over the past few weeks. Past medical history includes gestational hypertension for which she has been taking an antihypertensive drug twice daily since week 24. Her vital signs include: temperature 36.7°C (98.0°F), blood pressure 120/75 mm Hg, pulse 127/min. Physical examination reveals generalized pallor. Her laboratory results reveal microcytic, hypochromic anemia with anisocytosis, hemoglobin of 9 g/dL, a differential with 14% lymphocytes, an ESR of 22 mm/hr, and a reticulocyte production index of 3.1. A direct antiglobulin test is positive. LFTs, creatinine, ferritin level, vitamin B12 level, coagulation studies, and urinalysis are normal. Which of the following is the most likely diagnosis in this patient?\n",
        "\n",
        "A. Preeclampsia\n",
        "B. Hereditary spherocytosis\n",
        "C. HELLP syndrome\n",
        "D. Drug-induced immune hemolytic reaction\n",
        "\"\"\",\n",
        "            \"answer\": \"\"\"The patient's microcytic, hypochromic anemia and a high reticulocyte count are indicative of increased erythropoiesis, which, in this clinical scenario, is likely due to hemolysis. The positive direct antiglobulin test (Coomb's test) confirms that the hemolysis is immune-mediated. Given that other causes of immune hemolysis such as autoimmune disorders and infections are unlikely in this patient, the most probable cause of hemolysis is a drug-induced immune hemolytic reaction. Drugs can cause immune hemolysis by acting as haptens that bind to the red blood cell membrane, causing the formation of antibodies against the drug-cell complex (immune complex type) or by inducing the formation of autoantibodies against red blood cells (autoimmune type). While any drug can potentially cause immune hemolysis, common culprits include penicillins, cephalosporins, quinidine, and antihypertensives like methyldopa.\n",
        "\n",
        "Hence, the answer is [D].\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"\"\"You are counseling a pregnant woman who plans to breast-feed exclusively regarding her newborn's nutritional requirements. The child was born at home and the mother only plans for her newborn to receive vaccinations but no other routine medical care. Which vitamins should be given to the newborn?\n",
        "\n",
        "A. Folic acid\n",
        "B. Vitamin K\n",
        "C. Vitamin D\n",
        "D. Vitamin K and Vitamin D\n",
        "\"\"\",\n",
        "            \"answer\": \"\"\"The American Academy of Pediatrics (AAP) recommends that all newborns receive a single dose of intramuscular vitamin K at birth. This is because newborns have low stores of vitamin K at birth, leading to a risk of vitamin K deficiency bleeding, also known as hemorrhagic disease of the newborn. This is a potentially life-threatening condition that can cause bleeding into the brain. Because this baby was born at home and the mother plans to avoid routine medical care, this baby may not have received this essential vitamin.\n",
        "\n",
        "Additionally, the AAP recommends that all infants and children, including those who are breastfed, have a minimum intake of 400 IU/day of vitamin D beginning soon after birth. While breast milk is the best source of most nutrients, it is usually deficient in vitamin D.\n",
        "\n",
        "The other option, folic acid, is not routinely supplemented in newborns as they usually receive adequate amounts from breast milk or formula.\n",
        "\n",
        "Therefore, the answer is [D].\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"\"\"An investigator is studying nutritional deficiencies in humans. A group of healthy volunteers are started on a diet deficient in pantothenic acid. After 4 weeks, several of the volunteers develop irritability, abdominal cramps, and burning paresthesias of their feet. These symptoms are fully reversed after reintroduction of pantothenic acid to their diet. The function of which of the following enzymes was most likely impaired in the volunteers during the study?\n",
        "\n",
        "A. Methionine synthase\n",
        "B. Alpha-ketoglutarate dehydrogenase\n",
        "C. Glutathione reductase\n",
        "D. Dopamine beta-hydroxylase\n",
        "\"\"\",\n",
        "            \"answer\": \"\"\"Pantothenic acid is also known as vitamin B5, which is a component of coenzyme A (CoA) and phosphopantetheine. CoA is required for the synthesis and oxidation of fatty acids, and the metabolism of carbohydrates and proteins. One of the enzymes that requires CoA for its function is alpha-ketoglutarate dehydrogenase, a key enzyme in the citric acid cycle (also known as Krebs cycle or TCA cycle).\n",
        "\n",
        "This enzyme catalyzes the conversion of alpha-ketoglutarate to succinyl CoA, with the concomitant reduction of NAD+ to NADH. Deficiencies in pantothenic acid would impair the function of this enzyme, leading to the observed symptoms.\n",
        "\n",
        "So, the answer is [B].\"\"\",\n",
        "        },\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sf4xssj4sw6W"
      },
      "outputs": [],
      "source": [
        "# @title utility\n",
        "def extract_samples(task, numShot, model_prompt):\n",
        "    questions, answer_choices, correct_answers = task_load(task, 'train')\n",
        "    example_indexes = random.sample(range(len(questions)), numShot)\n",
        "    example_list = []\n",
        "    for i in example_indexes:\n",
        "        example_list.append(model_prompt.format(question=questions[i], choices=format_choices(answer_choices[i]), answer=correct_answers[i]))\n",
        "    return example_list\n",
        "\n",
        "def translate(Text,src,des):\n",
        "    Prompt = \"you are an expert translator in medical domain please translate the given\" + src + \"question to\" + des + \"please do not provide any additional information or explanations\" + '\\n' + \"text: \"\n",
        "    messages = [{\"role\": \"user\", \"content\": f\"{Prompt+Text}\"}]\n",
        "    inputs = translator_tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "         outputs = translator_model.generate(inputs, max_new_tokens=1024, do_sample = True, temperature=0.000000001)\n",
        "         text = translator_tokenizer.batch_decode(outputs)[0]\n",
        "    return text.split(\"model\")[-1].rstrip(\"<eos>\").strip()\n",
        "\n",
        "def resume_the_test(question_list, answer_choices_list, correct_answer_list,bgn):\n",
        "    question_list = question_list[bgn:]\n",
        "    answer_choices_list = answer_choices_list[bgn:]\n",
        "    correct_answer_list = correct_answer_list[bgn:]\n",
        "    for i, (question, answer_choices, correct_answer) in tqdm(enumerate(zip(question_list, answer_choices_list, correct_answer_list))):\n",
        "            context = \"\"\n",
        "            question = translate(question,\"persian\",\"english\")\n",
        "            if NSHOT == 0:\n",
        "               prompt = prompt_eval_bare_fully\n",
        "            else:\n",
        "               prompt = prompt_eval_bare_fully_with_examples\n",
        "\n",
        "            if NSHOT != 0:\n",
        "                examples = extract_samples(TASK, NSHOT, prompt_example)\n",
        "                model_prompt = prompt.format(\n",
        "                    question=question,\n",
        "                    choices=format_choices(answer_choices),\n",
        "                    examples = (\"\\n\").join(examples),\n",
        "                    context = filterContext(context)\n",
        "                )\n",
        "            else:\n",
        "                model_prompt = prompt.format(question=question, choices=format_choices(answer_choices), context = filterContext(context))\n",
        "\n",
        "            AI_answer = run_inference(model_prompt, ENGINE, ENGINE_TEMPERATURE, MAX_TOKEN_OUTPUT, tokenizer, model, local=True)\n",
        "            file_path = 'fa_pipeline_result.xlsx'\n",
        "            append_record_to_excel(file_path, question, answer_choices,\n",
        "                           correct_answer, model_prompt, AI_answer)\n",
        "\n",
        "            if i == STOP_GEN-1:\n",
        "                break\n",
        "\n",
        "\n",
        "def append_record_to_excel(file_path, Question, question_choices,\n",
        "                           correct_answer, model_prompt, AI_answer):\n",
        "    new_record = {\n",
        "        'Question': Question,\n",
        "        'question_choices': question_choices,\n",
        "        'correct_answer': correct_answer,\n",
        "        'model_prompt':  model_prompt,\n",
        "        'AI_answer': AI_answer\n",
        "    }\n",
        "    new_record_df = pd.DataFrame([new_record])\n",
        "    try:\n",
        "        existing_df = pd.read_excel(file_path)\n",
        "        updated_df = pd.concat([existing_df, new_record_df], ignore_index=True)\n",
        "    except FileNotFoundError:\n",
        "        updated_df = new_record_df\n",
        "\n",
        "    updated_df.to_excel(file_path, index=False)\n",
        "\n",
        "def format_choices(choices):\n",
        "    a = zip(list(choices.keys()), choices.values())\n",
        "    final_answers = []\n",
        "    for x,y in a:\n",
        "        y = translate(y,\"persian\",\"english\")\n",
        "        final_answers.append(f'[{x}] : {y}')\n",
        "    return \"\\n\".join(final_answers)\n",
        "\n",
        "def format_examples(examples):\n",
        "    formatted_examples = []\n",
        "    for row in examples:\n",
        "        example = f'## Question {row[\"question\"]} \\n ## Answer {row[\"answer\"]}'\n",
        "        formatted_examples.append(example)\n",
        "    return \"\\n\".join(formatted_examples)\n",
        "\n",
        "def get_ds_from_df(df,task):\n",
        "    if task == 'kopp':\n",
        "       df['Question'] = df['Question'].astype(str)\n",
        "       df['Option1'] = df['Option1'].astype(str)\n",
        "       df['Option2'] = df['Option2'].astype(str)\n",
        "       df['Option3'] = df['Option3'].astype(str)\n",
        "       df['Option4'] = df['Option4'].astype(str)\n",
        "       df['Topic'] = df['Topic'].astype(str)\n",
        "       df['Source'] = df['Source'].astype(str)\n",
        "       df['Correct answer'] = df['Correct answer'].astype(str)\n",
        "       ds = Dataset.from_pandas(df)\n",
        "       return ds\n",
        "    elif \"mmlu\" in task:\n",
        "       df['question'] = df['question'].astype(str)\n",
        "       df['option1'] = df['option1'].astype(str)\n",
        "       df['option2'] = df['option2'].astype(str)\n",
        "       df['Option3'] = df['option3'].astype(str)\n",
        "       df['option4'] = df['option4'].astype(str)\n",
        "       df['answer'] = df['answer'].astype(str)\n",
        "       ds = Dataset.from_pandas(df)\n",
        "       return ds\n",
        "    else:\n",
        "       raise Exception(\"TASK NOT FOUND\")\n",
        "\n",
        "def task_load(task, split):\n",
        "    if task==\"kopp\":\n",
        "        df = pd.read_excel(task+'.xlsx')\n",
        "        ds =get_ds_from_df(df,task)\n",
        "        questions = [ds[i]['Question'] for i in range(len(ds))]\n",
        "        answer_choices = [{\"A\": ds[i]['Option1'], \"B\": ds[i]['Option2'], \"C\": ds[i]['Option3'], \"D\": ds[i]['Option4']} for i in range(len(ds))]\n",
        "        correct_answers = [chr(int(ds[i]['Correct answer'])+64) for i in range(len(ds))]\n",
        "        return questions, answer_choices, correct_answers\n",
        "    elif \"mmlu\" in task:\n",
        "        df = pd.read_excel(task+'_fa.xlsx')\n",
        "        ds =get_ds_from_df(df,task)\n",
        "        questions = [ds[i]['question'] for i in range(len(ds))]\n",
        "        answer_choices = [{\"A\": ds[i]['option1'], \"B\": ds[i]['option2'], \"C\": ds[i]['option3'], \"D\": ds[i]['option4']} for i in range(len(ds))]\n",
        "        correct_answers = [chr(int(ds[i]['answer'])+64) for i in range(len(ds))]\n",
        "        return questions, answer_choices, correct_answers\n",
        "    else:\n",
        "        raise Exception(\"TASK NOT FOUND\")\n",
        "\n",
        "def filterContext(context):\n",
        "    end_tag = \"</end>\"\n",
        "    if end_tag in context:\n",
        "        return context.split(end_tag)[0] + end_tag\n",
        "    return context\n",
        "\n",
        "def run_inference(content, engine, temp=0.0001, max_tokens_output=1024, tokenizer=None, model=None, local=False):\n",
        "    if local:\n",
        "        messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
        "        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to('cuda:0')\n",
        "        with torch.no_grad():\n",
        "             outputs = model.generate(inputs, max_new_tokens=max_tokens_output, do_sample = True, temperature=temp)\n",
        "             text = tokenizer.batch_decode(outputs)[0]\n",
        "             return translate(text.split(\"<|assistant|>\")[-1],\"english\",\"persian\")\n",
        "    else:\n",
        "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "        messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=engine,\n",
        "            messages=messages,\n",
        "            temperature=temp,\n",
        "            max_tokens=max_tokens_output,\n",
        "            frequency_penalty=0.0\n",
        "        )\n",
        "        response_text = response.choices[0].message.content\n",
        "        return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "02b103b6a9194744890213b7422dbcb8",
            "15cea1a4380d49699b72f4f12a923bd3",
            "19f2d58420d646788cbed1426fff32f2",
            "1fcfb3de649a406eb974c462b1965dcc",
            "45683c9299374fc69a011df7e4a81abb",
            "c127ade208614d34b683a80cfcacbac6",
            "6698826b1e4244fe927a32a6c1ad9333",
            "ef5562b25d1a4decbddefbb8900bba38",
            "2c0eae9c2f8f4395bab6db930acd43dd",
            "98fa74952aec4666a824f292ac584349",
            "ce58c562da1045979a8d0177255bb58a",
            "42b74b9ab4dc4af4926d6a338896925b",
            "032da2338fe34226bfe7dac477ca1db5",
            "9979d1baf9c84f1fac7089426d5ec448",
            "f266eccaf3dd487f9a387f989921f0ac",
            "568b3274534f44c6a8605dfee1184a8f",
            "29719b7c2c5c4de5bbf2ac0bf6d5ecc4",
            "cbb716f6e2d9454a90f5c483bffbebba",
            "4758fff11dd64f79817fe2d8ec39b482",
            "89d6a292379e42558660990257475bd1",
            "da132aa3834e41418cf6d751a53b71d4",
            "a9179fd7034b48048e41df3b73adab5d",
            "314462d42552491aaf28f92b48cbc6ad",
            "e7a2aa9e91bf4124a9ce21aeb96043ee",
            "8a592851cb104a688f6ef1f629245a1a",
            "467015ddb0db42418abd6d85d1441bc9"
          ]
        },
        "id": "bfwb6Cd_epyG",
        "outputId": "8b7b0295-f9eb-4194-f97a-d92888ae455b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUNNING NORMAL IMPLEMENTATION\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02b103b6a9194744890213b7422dbcb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15cea1a4380d49699b72f4f12a923bd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19f2d58420d646788cbed1426fff32f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fcfb3de649a406eb974c462b1965dcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45683c9299374fc69a011df7e4a81abb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c127ade208614d34b683a80cfcacbac6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6698826b1e4244fe927a32a6c1ad9333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5562b25d1a4decbddefbb8900bba38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c0eae9c2f8f4395bab6db930acd43dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98fa74952aec4666a824f292ac584349",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.33k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce58c562da1045979a8d0177255bb58a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42b74b9ab4dc4af4926d6a338896925b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "032da2338fe34226bfe7dac477ca1db5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9979d1baf9c84f1fac7089426d5ec448",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/447 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f266eccaf3dd487f9a387f989921f0ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "568b3274534f44c6a8605dfee1184a8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29719b7c2c5c4de5bbf2ac0bf6d5ecc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbb716f6e2d9454a90f5c483bffbebba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4758fff11dd64f79817fe2d8ec39b482",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89d6a292379e42558660990257475bd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da132aa3834e41418cf6d751a53b71d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9179fd7034b48048e41df3b73adab5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "314462d42552491aaf28f92b48cbc6ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7a2aa9e91bf4124a9ce21aeb96043ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a592851cb104a688f6ef1f629245a1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "467015ddb0db42418abd6d85d1441bc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Running: KrithikV/MedMobile\n"
          ]
        }
      ],
      "source": [
        "# @title model setting\n",
        "\n",
        "print(\"RUNNING NORMAL IMPLEMENTATION\")\n",
        "ENGINE = \"KrithikV/MedMobile\"\n",
        "SPLIT = \"test\"\n",
        "ENGINE_TEMPERATURE = 0.000000001\n",
        "MAX_TOKEN_OUTPUT = 1024\n",
        "NSHOT = 0\n",
        "STOP_GEN = 10000000 ## For testing purposes; stop generating after {STOP_GEN} amount of test-questions\n",
        "TASK = 'kopp' # Options [\"medqa\", 'mmlu-anatomy', 'mmlu-professional_medicine', 'mmlu-college_biology', 'mmlu-college_medicine', 'mmlu-clinical_knowledge', 'mmlu-medical_genetics', medmcqa\"]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "## LOAD IN MODEL IF LOCAL\n",
        "model_path = ENGINE\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"cuda\",torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        ENGINE,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\")\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(ENGINE)\n",
        "\n",
        "translator_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "translator_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b-it\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(device)\n",
        "translator_model.eval()\n",
        "## OUTPUT RUN INFO:\n",
        "print(\"Model Running: \" + ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the test\n",
        "question_list, answer_choices_list, correct_answer_list = task_load(TASK, SPLIT)\n",
        "print(f\"{TASK} loaded succesfully. Now conducting evaluation on {len(question_list)} samples.\")"
      ],
      "metadata": {
        "id": "4rEV9mk4BBaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv7SqXHd_Ghc",
        "outputId": "832481d9-b945-433f-88e3-5afe5c5bd8d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "168it [52:48, 18.86s/it]\n"
          ]
        }
      ],
      "source": [
        "# Since google colab usage time is limited & this test takes days to complete\n",
        "#  we need to concatenate the result of many session to get the final result\n",
        "#   so set the bgn variable to number of question that has been solved in previous sessions\n",
        "resume_the_test(question_list, answer_choices_list, correct_answer_list,bgn = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZSJvzzze17v",
        "outputId": "a70aa269-cb94-4006-85bb-baecbbfd76c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Questions: 168\n",
            "#Correct answers: 41\n",
            "#Invalid answers: 64\n",
            "Accuracy: 0.24404761904761904\n"
          ]
        }
      ],
      "source": [
        "# append AI_chosen_answer column manually to the excel file\n",
        "#  then measure the accuracy\n",
        "df = pd.read_excel('fa_pipeline_result.xlsx')\n",
        "number_of_questions = 0\n",
        "number_of_invalid_answers = 0\n",
        "number_of_correct_answers = 0\n",
        "for index,row in df.iterrows():\n",
        "    number_of_questions += 1\n",
        "    AI_chosen_answer = row['AI_chosen_answer']\n",
        "    correct_answer = row['correct_answer']\n",
        "    if AI_chosen_answer == 'invalid':\n",
        "       number_of_invalid_answers += 1\n",
        "    elif AI_chosen_answer == correct_answer:\n",
        "       number_of_correct_answers += 1\n",
        "print('#Questions: '+str(number_of_questions))\n",
        "print('#Correct answers: '+str(number_of_correct_answers))\n",
        "print('#Invalid answers: '+str(number_of_invalid_answers))\n",
        "print('Accuracy: '+str(number_of_correct_answers/number_of_questions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}